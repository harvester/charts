# Default values for harvester-lvm-csi-driver.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

pluginImage:
  repository: rancher/harvester-lvm-csi-plugin
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "v0.1.2"

provisionerImage:
  repository: rancher/harvester-lvm-provisioner
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "v0.1.2"

hookImage:
  repository: rancher/kubectl
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "v1.32.3"

nameOverride: ""

lvm:
  # You will want to change this for read-only filesystems
  # For example, in Talos OS, set this to "/var/etc/lvm"
  hostWritePath: /etc/lvm

  driverName: lvm.driver.harvesterhci.io

webhook:
  replicas: 1
  image:
    repository: rancher/harvester-lvm-csi-driver-webhook
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: "v0.1.2"
  httpsPort: 8443

rbac:
  create: true

kubernetes:
  kubeletPath: /var/lib/kubelet

storageClasses:
  linear:
    enabled: true
    additionalAnnotations: []
    # this might be used to mark one of the StorageClasses as default:
    # storageclass.kubernetes.io/is-default-class: "true"
    reclaimPolicy: Delete
  striped:
    enabled: true
    additionalAnnotations: []
    reclaimPolicy: Delete
  mirror:
    enabled: true
    additionalAnnotations: []
    reclaimPolicy: Delete

customCSISidecars:
  enabled: false

  ## uncomment and set these if enabled=true

  # attacher: registry.k8s.io/sig-storage/csi-attacher:v4.4.2
  # livenessprobe: registry.k8s.io/sig-storage/livenessprobe:v2.12.0
  # provisioner: registry.k8s.io/sig-storage/csi-provisioner:v3.6.2
  # registrar: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.2
  # resizer: registry.k8s.io/sig-storage/csi-resizer:v1.9.2
  # snapshotter: registry.k8s.io/sig-storage/csi-snapshotter:v6.3.4

nodeSelector:
  # The plugin daemonset will run on all nodes if it has a toleration,
  # so it is not necessary to set a nodeSelector for it

  # plugin:
    # node-role.kubernetes.io/master: ""
    # Key name may need to be updated to 'node-role.kubernetes.io/control-plane'
    # in the future

  # The provisioner has an affinity for nodes with a plugin pod,
  # but since that's a daemonset, we allow more fine-grained node selection

  provisioner:
    # node-role.kubernetes.io/master: ""
    # Key name may need to be updated to 'node-role.kubernetes.io/control-plane'
    # in the future

tolerations:
  plugin:
  # - key: node-role.kubernetes.io/master
  #   operator: Exists
  #   effect: NoSchedule
  # - key: node-role.kubernetes.io/control-plane
  #   operator: Exists
  #   effect: NoSchedule
  provisioner:
  # - key: node-role.kubernetes.io/master
  #   operator: Exists
  #   effect: NoSchedule
  # - key: node-role.kubernetes.io/control-plane
  #   operator: Exists
  #   effect: NoSchedule
  webhook:
  # - key: node-role.kubernetes.io/master
  #   operator: Exists
  #   effect: NoSchedule
  # - key: node-role.kubernetes.io/control-plane
  #   operator: Exists
  #   effect: NoSchedule
